\documentclass[12pt, a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{amsmath}% http://ctan.org/pkg/amsmath
\usepackage{array}
\usepackage{pgfplots}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[a4,frame,center]{crop}
\usepackage[margin=1in,left=1.5in,top=1in, includefoot]{geometry}
\usepackage{chngcntr}
\counterwithout{figure}{chapter}
\counterwithout{equation}{chapter}
%Header and Footer package
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\renewcommand{\headrulewidth}{1pt}
\renewcommand{\footrulewidth}{0pt}
\lhead{Implementation of Intrusion Detection System in Tensorflow}
\rfoot{Page \thepage}
\usepackage{ragged2e}

\begin{document}

\begin{titlepage}
	\begin{flushleft}
	\includegraphics[width=14cm]{combine.png}\\
	\end{flushleft}
	\begin{center}
	
	
	\Large\bfseries{Otto-Von-Guericke University Magdeburg}\\
	[0.5in ]
	\large {Faculty of Electrical Engineering and Information Technology}\\
	[0.5in]
	\large {Institute for Automation Engineering}\\
	[0.5in]
	\large{Chair for Integrated Automation}\\
	[0.5in]
	\normalsize{\underline {Digital Engineering Project (12 Credits)}}\\
	[0.3in]
	\Huge\bfseries {Implementation of Intrusion Detection System in Tensorflow}\\
	[1in]
	\normalsize {\underline {Supervisor}}\\
	[0.2in]
	Dipl.-Ing. Sasanka Potluri\\
	[0.6in]
	\normalsize {\underline{Project by}}\\
	[0.2in]
	Jay Vala (Matriculation Number - 213739)\\
	Akash Antony (Matriculation Number - 213739)\\
	Kartik Sareen(Matriculation Number - 213739)\\
	\end{center}
	
\end{titlepage}

\thispagestyle{empty}
\tableofcontents 
\listoffigures
\listoftables



\begin{abstract}
\pagenumbering{arabic}
\pagestyle{empty}
\justify
The increasing number of cyber threats in industrial environments has prompted pioneers to search for security solutions that can protect and prevent industries from significant monetary loss and brand erosion. \\ \par
The cyber attacks on Davis-Besse Power Station in January, 2003 and on Iranian Nuclear Plant in December, 2010 has turned industries into finding new and effective ways to tackle cyber security threats in industrial automation. According to Kaspersky lab \emph ``Every month, an average of one industrial computer in five (20.1\%) is attacked by malware.''\cite {Kaspersky}\\ \par
With the rapid adaption and expansion of artificial neural networks for various information and data processing, it is imminent that it will find its use in tackling cyber security. As artificial neural networks are becoming powerful day by day, its power can be harnessed to find the anomalies in industrial network traffic.\\ \par
Stacked Autoencoder and Deep Belief Networks are such artificial neural network which are used for unsupervised learning. A Stacked Autoencoder and Deep Belief Network are multi-layer deep artificial neural networks (DNNs). DNNs are multiple layer architecture which extracts inherent features in data and discover important hidden structure in diverse data sets\cite{abs_auto}. This makes them suitable for classification tasks. \\ \par
In this project we have created Stacked Autoencoder and Deep Belief Network using Tensorflow, Google's open-source software library for Machine Intelligence. We have used NSL-KDD dataset as it solved some of inherent problems of the KDD'99 dataset. We have applied greedy layer-wise training approach to train our models. We have also used different learning parameters such as different activation functions, different optimizers and variables batch size, there is a command line interface where anyone can tweak these and many more parameters to get optimum results.\\
\end{abstract}
\cleardoublepage

\setcounter{page}{1}

\chapter{Introduction}\label{sec:intro}
\justify
With the exponential increase in computer network usage and the increasing number of devices getting connected to a network, it is of great importance that they are operated securely. All computer networks suffers from one of many security flaws, the recent ``Wannacry Ransomware'' took cyber security industry by storm. Though there was a fix for that security loophole, organizations were lazy on applying the security patches, this behaviour of the organization can be because of organizational structure, increasing profitability or in some cases the machine is operating a critical infrastructure. Therefore, Intrusion Detection System's (IDS) role is becoming important in tackling cyber security.\\ \par

Recent advances in machine learning and artificial intelligence has opened up many new possibilities. Machine learning today is used in Advance Medical Research, Self Driving Cars, Image Recognition and many more. Machine learning and Artificial Intelligence are used in classification and this is why it makes machine learning algorithms suitable for anomaly detection in network traffic. \\

\section{Intrusion Detection System}\label{sec:introintrusion}

\justify
Intrusion detection system can be compared with a burglar alarm \cite{IDS}. For example the alarm system at a house is to protect a house. If somebody who is not authorized to enter the house enters, it is the burglar alarm that detects and informs the owner of the intrusion in the house or if need be it locks the house. Intrusion detection system in a similar way compliments the firewall security. The firewall protects institution from malicious attacks and it is the job of the Intrusion detection system if someone bypasses the firewall and try to break into or tries to access the system  to alert the network administrator in case of security breach.\\ \par

However, Firewalls do a great job of filtering out the unwanted incoming traffic into the industrial or institutional network but there may be some cases where external users can connect to the Intranet by dialling in through a modem installed in the private network of the organization. This kind of access would not be detected by a firewall.\cite{IDS}

So, an Intrusion Detection Systems (IDS) monitors and analyses the network traffic and monitors it for anomalies and abnormalities and alerts the system or the system administrator of the attacks originating from outside and also from the system misuse or attacks originating from inside the organization.\\ \par

The attacks may be of various types and the system should be able to predict and identify the attack accurately, without giving false positives, hence the accuracy of the system becomes more important. \\ \par

\section{Classification of Intrusion Detection System}

Intrusion detection systems are classified into two sub categories, Network Intrusion Detection System (NIDS) and Host-Base Intrusion Detection System (HBIDS). Network Intrusion detection systems are placed at strategic points within the network where it can monitor traffic coming from and going to different devices in the network. If the network is large enough this can be a bottleneck that would directly affect the network speed. On the contrary, host based intrusion detection systems are run on individual host or nodes in the system so it only monitors the inbound and outbound network packets from that particular host.\\

These intrusion detection systems are also classified on the bases of how they detect intrusion in the first place. Signature based intrusion detection system will monitor all the packets in an network and compare it against the signature of the previously known threats in a database. This is similar to how most of the antivirus works, but the issue here is that there is a lag between a new threat being discovered and its signature being recorded in the database. During that time the system will be vulnerable to that threat.\\

Anomaly based intrusion detection are primarily used for detecting unknown attacks on a system, where it  will also monitor the network packets and compare it with a established good set of attributes in the network. System parameters such as bandwidth, protocol, ports and others are analysed against a good previously known attributes. This approach enables us to identify the previously unknown threats but suffers from false positive results, that is it may identify a normal activity as an attack.\\ \par 
\section{Advantages of Network Based Intrusion Detection System}\label{sec:advantagesofNIDS}
\begin{enumerate}
	\item {Low Cost of Ownership}
	\item {Easier to deploy}
	\item {Detect network based attacks}
	\item {Retaining evidence}
	\item {Real Time detection and quick response}
	\item {Detection of failed attacks}
\end{enumerate}

\section{Advantages of Host Based Intrusion Detection System} \label{sec:advantagesHostBasedIDS}
\begin{enumerate}
	\item{Verifies success or failure of an attack}
	\item{Monitor System Activities}
	\item{Detects attacks that network based IDS fails to detect}
	\item{Near real time detection and response}
	\item{Does not require additional hardware}
	\item{Low entry cost}
\end{enumerate}


\section{Limitations of Intrusion Detection Systems}\label{sec:limitsofIDS}

Although very powerful these systems also suffer from many limitations. Noise such as bad packets generated from software bugs, corrupted DNS data can create false alarms \cite{securityengineering}. Outdated software versions are generally the target of the attackers, hence a constantly updated signature database has to be maintained in order to tackle these threats \cite{securityengineering}. Also for signature based Intrusion Detection system there is a lag between a new threat detected and the signature being recorded in the database, this can make IDS vulnerable for that amount of time. \\ \par

Taking authentication and identity confirming mechanism into consideration, IDS are helpless against weak authentication when an attacker gains access. A weakness in the network protocol also makes IDS vulnerable. Encrypted packets are not processed by the IDS and this makes it easy for an attacker to send in encrypted packets and gain access to the system. As NIDS analyse the same network protocol as the attacker uses to attack the system, NIDS on its own can become vulnerable and susceptible to such attacks. \cite{limitationOfIDS}
\clearpage



\chapter{Tensorflow}\label{sec:tensorflow}

\section{Introduction}\label{sec:tensor_intro}
\justify
Tensorflow is an interface for expressing machine learning algorithms and an implementation for executing such algorithms. A computation expressed using Tensorflow can be expressed with little or no changes on wide variety of heterogeneous systems \cite{tensorflow_paper}. \\ \par
\justify
The Google's Brain project built DistBelief, the first-generation scalable distributed training and inference system. With all the understanding and DistBelief as its core, Google's Brain Team built Tensorflow the second generation of machine learning library. Google and other Alphabet companies have deployed DistBelief and Tensorflow into many of Google's products including Google's search \cite{googleSearch}, Google's Advertising Production, Google's Speech Recognition, Maps, Photos and many more. \\ \par
\justify
Tensorflow is highly scaleable and can be mapped on many devices and platform ranging from Android and iOS to large scale system consisting of multi-cpu and multi-gpu configurations. For scaling neural network training to larger deployment, Tensorflow allows parallelism through replication and parallel execution of core model with different computational devices all collaborating to update a set of shared parameters or other states. Modest changes in the description of the computation allows a wide variety of different approaches to parallelism with low effort. Some uses of Tensorflow allows some flexibility in terms of the consistency of parameters updates and we can easily express and take advantage of these relaxed synchronization requirements in some of large deployments.\\ \par

A computation model in Tensorflow is called a \textit{graph}, which in turn composes of set of \textit{nodes}. The graph represents the flow of computation with extension for allowing some kind of nodes to maintain and update its states. Tensorflow supports two front end languages C++ and Python to construct these graphs.\\ \par
\clearpage

\section{Graph Construction and Basic Programming Concept}\label{sec:programmingtensorflow}

As mentioned in section \ref{sec:tensor_intro} a Tensorflow model consists of a \textit{graph} which is composed of \textit{nodes}. These \textit{nodes} has zero or more inputs and zero or more outputs. Values that flow along the edges are called \textit{tensors}. They are nothing but arbitrary dimensional arrays.\\ \par

\subsection{Operations, Kernels, Variables, and Sessions}\label{sec:constituentsoftensorflow}

\subsubsection{Operations and Kernels}\label{sec:ops_kernel}
An operation in its simplest of meaning is calculation of inputs to form output. In Tensorflow, calculation such as ``matrix multiplication'' or ``addition'' are operations. An \textit{operation} in Tensorflow has name and attributes which have to be provided at the time of graph construction. \\ 

A \textit{kernel} is a particular implementation of an operation that can be run on a particular type of devices like CPU or GPU. 


\subsubsection{Variables}\label{variables}
A computation graph is executed many times for a single model, most tensors do not have a persistent state and looses its state after a single execution. Hence, \textit{Variable} which are persistent, mutable handles for storing tensors.


 \subsubsection{Session}\label{session}
A user interact with Tensorflow system by creating a \textit{Session} which creates the computation graph. One primary operation supported by \textit{Session} is \textit{Run} which takes set of output names that needed to be computed, as well as an a set of tensors to be fed to the computational graph and operations are performed accordingly.

\subsection{Tensor}\label{tensor}

A tensor as mentioned above, is an multidimensional array. Tensorflow supports variety of tensor elements including signed and unsigned integers ranging from 8bits to 64bits.

\clearpage

\chapter{Deep Neural Network}\label{ref:DNN} 

\section{Introduction}\label{sec:intro_dnn}

A Deep Neural Network (DNN) is an Artificial Neural Network (ANN) with multiple hidden input and output layers. Similar to other ANNs, DNNs can model complex non-linear relationships. Because each hidden layers computes a non-linear transformation of previous layer, a DNN can have significantly greater power of representation. DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. However, recurrent neural networks, in which data can flow in any direction, are also used, especially long short-term memory, for applications such as language modeling. Convolutional deep neural networks (CNNs) are used in computer vision But before understanding the architecture and workings of a Deep Neural Network we must understand what an ANN is. \\ \par

\section{Artificial Neural Network}\label{ANN}
An Artificial Neural Network (ANN) are information processing units that are inspired by the way a human nervous system, such as brain, process information. As human brain processes information and takes decision, these models are designed to do the same. Human brain learns from its day to day experience, these ANN try and replicate the learning process, which makes them suitable for some of the unsolvable and resource hungry task. The key element of these neural networks is a Neuron, this neuron is responsible for all the mathematical operations that are performed such as summation, multiplication etc. When these neurons are highly interconnected in large numbers, they work in unison to solve parts of a bigger problem.\\ \par

ANNs, like human beings learn by examples. An ANN is configured for a specific task or application. These task varies from patten recognition in computer-aided diagnosis, speech recognition to classification of data such as text into various categories. Today ANNs find its use in verity of application, Google's search are optimized by these neural networks, Facebook's predictive algorithm in news feeds to Amazon's virtual assistant Alexa. \\ \par

Neural Network with their ability to derive meaning from complicated and huge data, that would have been otherwise impossible for either humans or other computer techniques, is what makes it powerful tool. Other advantages include Adaptive Learning, Self Organization, and Fault Tolerance.\\ \par

An artificial neuron is the building block of an ANN. Every input is multiplied with random weights so that the inputs are weighted. These weighted inputs are then summed up with biases and passed through a transfer function and then the net input is passed through the activation function as shown in the figure \ref{fig:AN} \\ \par
\begin{figure}[h]
\centering	
\includegraphics[width=12cm]{ArtificialNeuronModel.png}\\
\caption{Structure of Artificial Neuron}
\label{fig:AN}
\end{figure} 	

Although the structure and computation of a single artificial neuron looks simple and easy, but its full potential and power of calculation is realized when they are interconnected and made into an artificial neural network as show in figure \ref{fig:NN}\\ \par
\begin{figure}[h]
\centering
\includegraphics[width=12cm]{neural_network.png}
\caption{Structure of Artificial Neural Network}
\label{fig:NN}
\end{figure}

These artificial neural networks are interconnections of artificial neurons, however these neurons can not be connected however we want. There are predefined architectures and topologies which make them more beneficial to use. These architectures are very effective in numerous tasks and can solve problems quickly and effectively. Once we know the type of problem we need to solve, then we can select appropriate  architecture or topology for the artificial neural network and run our model.\\ \par

Once the appropriate topology is selected, then we are ready to train our model. Just like a human brain which learns its responses from the inputs  from all the sensory organs, for an artificial neural network to do whatever its intended to do it requires training. There are two phases of training, one is pre-training and another is fine tuning phase. As ANN is successful in solving the problems we have provided, then it can be used for solving the problem we intended. Artificial neural networks are used in the fields of chemistry, medicine, banking, stock market. It can also solve problems like facial recognition, time-series prediction, regression analysis, pattern recognition, and clustering.\\ \par

\section{Artificial Neural Networks: Types}

As we know the building block of Artificial Neural Network (ANN) is the artificial neuron, and when we have more then two artificial neurons working together, we have an artificial neural network. A single neuron is not powerful enough to solve big complex real world problems, but when they are put together in a specified topology or architecture they can be very handy in solving the same task a single neuron fails to perform. Advantage of artificial neural network is that these neurons can process data in distributed way, in parallelly, non-linearly and locally. \\\par

The ways in which these neurons are connected is what makes them so powerful. There are basically two main topologies in which these neurons are connected. 
\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth,height=4cm]{nn_2.png}
    
  \end{subfigure}
  %
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=9cm,height=4cm]{rnn.png}
  \end{subfigure}
  \caption{Feed Forward and Recurrent Neural Network}
 \label{fig:FFRN}
\end{figure}

As we can see from the figure \ref{fig:FFRN} that in feed forward neural network the flow of information is only in one direction that is from inputs to the outputs wherein in the recurrent neural network the information flows in many possible directions as show in the figure \ref{fig:FFRN} \\ \par

There are other neural networks as follows
\begin{itemize}
	\item Convolutional Neural Networks
	\item Long sort-term memory
	\item Deep Belief Networks
	\item Stacked Auto-encoders
	\item Hopfield Neural Networks 
	\item Elman and Jordan Artificial Neural Networks
	\item Generative Adversarial Networks
	\item Boltzmann and Restricted Boltzmann Machines
\end{itemize}
\section{Training A Deep Neural Network}\label{train}

Training a neural network \textit{Deep or Shallow} requires some parameters to be set. These parameters are important in a way that is it determines the way a neural network will be producing results. Once the structure of the network is decided that is how many layers would be needed for that specific task and what will be the number of neurons in each layer, will you be needing a shallow or a deep network. After the structure of the neural network is deicided upon then it has to be decide what \textit{activation function} is to be used, what will be the learning algorithm to be used, how many epochs will be needed, what kind of learning will be needed, will it be supervised, unsupervised or reinforcement learning. These parameters will decide what kind of output will it be producing.\\ \par

\subsection{Activation Functions}\label{activationfunction}
Activation function in a neural network can be described as a function which transfers or translates the input signal to output signal. Comparing it to electrical circuit an activation function is one which can be ``\textit{ON}'' (1) or ``\textit{OFF}'' (0) depending on the input it receives. As the artificial neurons are inspired by the biological neuron, the activation function in biological from is either the neuron is firing or not.\\ \par

Activation functions are selected for a specific application and are highly application dependent. There are many applications which needs classification and there are many application which require prediction, hence the properties of different activations are used in different applications. The most common activation functions with its properties are listed below. However we have used only four of the activation functions namely \textit{Sigmoid Activation, ReLU activation, eLU activation, Softplus activation} and in all the models the last layer of the model has \textit{Softmax activation}.\\ \par

\subsubsection{Sigmoid Activation Function}\label{sec:sigmoid}
Sigmoid is one of the most common activation function used in neural networks. This function is widely used because its easy to calculate their derivatives, which makes weights calculation very easy in some cases. Sigmoid activation has the vanishing gradient problem.\\ \par
%equation of sigmoid
\begin{equation}
S(x) = \frac{1}{1-e^{x}} \\
\label{eq:sig}
\end{equation}

%graph of sigmoid
\begin{figure}[h]
\centering	
\includegraphics[width=12cm]{sigmoid.png}\\
\caption{Sigmoid Activation Function}
\label{fig:sigmoid}
\end{figure} 	

\subsubsection{Step Function}\label{sec:step}
Step function was used in perception. The output is certain value, if the sum of the inputs is above or below a certain threshold. These kind of activation functions were used in case of binary classification. In other words, if there are only two classes to classify we can use this activation function.\\ \par
%equation of step
 \begin{equation}\label{eq:step}
f(x) = \begin{Bmatrix}
1 & x \in A \\ 
0 & x \notin A 
\end{Bmatrix}
\end{equation}

%graph of step
\begin{figure}[h]
\centering	
\includegraphics[width=5cm]{step.png}\\
\caption{Step Activation Function}
\label{fig:step}
\end{figure} 	

\subsubsection{Rectified Linear Unit}\label{sec:ReLU}
Rectified Linear Unit function was first introduced by Hahnloser in 2000 \cite{ReLU} with strong biological motivation and justification. This is the most popular activation function from deep neural networks.\\ 
%equation of relu
\begin{equation}\label{eq:ReLU}
f(x) = \begin{Bmatrix}
0 & x < 0 \\ 
x & x \geq 0 
\end{Bmatrix}
\end{equation}
%graph of relu
\begin{figure}[h]
\centering
\includegraphics[width=6.5cm]{relu.png}
\caption{Rectified Linear Unit Activation Function}
\label{fig:ReLU}
\end{figure} 

\subsubsection{TanH Activation Function}\label{sec:tanh}
TanH activation function has characteristics similar to sigmoid activation function, its a scaled sigmoid. It is nonlinear in nature, and its gradient is stronger than sigmoid. Deciding on which to use will depend on the application. Like sigmoid, TanH also has the vanishing gradient problem.
%eq 
\begin{equation}\label{eq:tanh}
f(x) = tanh(x)=\frac{2}{1+e^{-2x}} -1
\end{equation}
%fig
\begin{figure}[h]
\centering
\includegraphics[width=6.5cm]{tanh.png}
\caption{TanH Activation Function}
\label{fig:tanh}
\end{figure} 

\subsubsection{Exponential Linear Unit}\label{sec:ELU}
Exponential Linear Unit alleviate the vanishing gradient problem via the identity for positive values. However, Exponential Linear Units have improved learning characteristics compared to the units with other activation functions \cite{ELU}.\\ \par
%formula
\begin{equation}\label{eq:ELU}
f(\alpha ,x) = \begin{Bmatrix}
\alpha(e^{x}-1) & x<0\\ 
x & x\geq 0 
\end{Bmatrix}
\end{equation}

\subsubsection{Softmax Activation Function}\label{sec:softmax}
Softmax activation function is a generalization of the logistic function, in probability theory the output of the softmax function can be used to represent a categorical distribution, that is a probability distribution over \textit{X} possible outcomes. Hence, softmax activation function is used in various multi-class classification in artificial neural networks.\\ \par
%equation forsoftmax
\begin{equation}\label{eq:softmax}
f_{i} (\overrightarrow{x}) =\frac{e^{x_{i}}}{\sum_{j=1}^{J} e^{x_{i}} }     \forall \textit{i=1...J}
\end{equation}
\\ \par

%optimizers
\subsection{Optimization Algorithm}\label{optimizer}

Optimization algorithms in neural networks helps in minimizing or maximize a loss function which is simple a mathematical function dependent on models internal parameter which calculates models inability in computing the target values from the set of predictions. The internal parameters such as weights and biases values of a neural network which plays a major role in training process of the neural network, optimizing algorithm helps in setting the values such that there is practically no difference between the predicted value and the value that was feed to the network.\\ \par

In this project of ours we have used four different optimizers to minimize the loss function, namely \textit{Adam Optimizer, Stochastic Gradient Descent optimizer, RMSProp Optimizer, Momentum Optimizer}

\subsubsection{Stochastic Gradient Descent}\label{sgd}

It is a stochastic approximation of the gradient descent optimization method for minimizing an objective function that is written as a sum of differentiable functions. In other words, Stochastic Gradient Descent (SGD) tries to find minima or maxima by iteration.



\section{Challenges in Training A Deep Neural Network}\label{challenges}

Deep Neural Networks are comprised of multiple hidden layers, so many issues can arise while training a DNN. Two most common issues are \textit{Overfiting} and \textit{Computation time.}\\

DNNs are prone to overfitting because of the added layers of abstraction, which allows them to model rare dependencies in the training data. Regularization method such as weigh decay (l2- regularization) or sparsity (l1 regularization) can be applied during the training of the model to combat overfitting.\\\par

Training a DNN with multiple hidden layers consisting of hundreds of thousands of neurons can be a lengthy and time consuming process. Considering many training parameters such as size, the learning rate, initial weights,biases and trying each and every possible combination can be a time consuming process and may not even be a feasible process. Various tricks such as batch size can speed up the computation process. Use of GPS can also speed up the process.

\section{Stacked Autoencoder}\label{Autoencoder}
An autoencoder is an simple artificial neural network first introduced in 1980 by Hilton and the PDP group \cite{autoencoder}. Autoencoders are used for unsupervised learning. They were introduced to address the problem of ``backpropagation without teacher'', by using inputs as teacher \cite{autoencoder2}. The aim of an autoencoder is to learn a representation of a data. Recently, the autoencoders have become widely popular and used for learning generative models in unsupervised pre-training.

\subsection{Structure}\label{structure}

In its simplest form an autoencoder is a feedforward, non recurrent model. It can be generalized as a network having input layer, output layer and hidden layer or layers. A stacked autoencoder is nothing but a network we get by stacking hidden layers onto one another and making it a deep architecture. The output layer has same number of nodes as the input layer with the purpose of reconstruction of the inputs. Inour case we had to classify the attacks in a network with five class hence in our case there will five neurons in the output layer.\\ \par

In simplest of the cases, where there is only one hidden layer the mathematical representation of autoencoder will be,
\begin{gather}\label{eq:autoencoder_eq}
y = \sigma (Wx+b)
\intertext{Where:}
  \begin{tabular}{>{$}r<{$}@{\ :\ }l}
    y & is the output of the network\\
    \sigma & is the activation or the transfer function\\
   W& is the weights values   \\
    x & is the values of input signal \\
    b & is the values of bias added \\
  \end{tabular}\nonumber
\end{gather}
\\ \par
\begin{figure}[h]
\centering
\includegraphics[width=7cm]{autoencoder.png}
\caption{Autoencoder with single hidden layer}
\label{fig:autoencoder}
\end{figure}

\subsection{Types of Autoencoders}\label{sec:types_autoencoder}

The simple autoencoder have some disadvantages, in some cases it was unable to learn the important features effectively. Hence, there was a need for improving the ability of autoencoders to learn the important features and representations for highly accurate results.\\ \par

\subsubsection{Denoising Autoencoders}\label{sec:denoising_autoencoder}

When a neural network like autoencoder is trained, its kept in mind that the inputs provided for training purpose are not corrupted. After the training is done and the model is applied to a real world problem there is a possibility that the inputs given to autoencoder are corrupted, this may destabilize the model, and the results may not be robust. In order to tackle this problem, in \textit{Denoising autoencoder} takes partially corrupted inputs in the training phase to recover the uncorrupted inputs.\\ \par

\subsubsection{Sparse Autoencoder}\label{sec:sparse_autoencoder}

In \textit{Spares autoencoder}, we have large number of hidden units, more than inputs, by doing so the autoencoder can learn more useful structures and representation of the data. This kind of autoencoder is useful in classification tasks. Also sparsity can be achieved by tweaking the loss function during training (comparing probability distribution of the hidden unit activation with some low desired values) or by manually choosing the strongest hidden activation functions and ignoring the rest refereed to as \textit{k-sparse autoencoders}\cite{ksparseautoencoder}.\\ \par


\section{Deep Belief Networks}\label{sec:deep_belief_network}

Deep Belief Networks are probabilistic generative models that are composed of multiple layers of stochastic, latent variables. The latent variables are binary units and are often called feature detectors\cite{DBN}. There is connection between units of different layers but not within the layers.
A deep belief network is stacked \textit{Restricted Boltzmann Machines} (RBM), in which each RBM layer communicates with previous and subsequent layer, but the nodes of any single layer don't communicate with each other.

\subsection{Restricted Boltzmann Machine}\label{sec:RBM}

A \textit{restricted boltzmann machine}(RBM) is a generative stochastic artificial neural network that can learn a probability distribution over a set of inputs. RBMs were invented by Harmonium by Paul Smolensky in 1986\cite{RBM}. RBMs have found their uses in dimentionality reduction, classification, collaborative fitting, feature learning and topic modelling\cite{RBMuses}. They are variant of Boltzmann Machines, with the restrictions that their neurons must form a bipartite graph, and nodes in same layer have no connection between themselves. In figure \ref{fig:RBM}, \textit{h} is the hidden units and \textit{x} is the inputs.\\ \par

\begin{figure}[h]
\centering
\includegraphics[width=9.5cm]{DBN.png}
\caption{Deep Belief Network}
\label{fig:RBM}
\end{figure}
\clearpage

\chapter{Evaluation and Results}\label{sec:EvaluationAndResults}
\section{Approach}\label{approach}

\section{Stacked Autoencoder}\label{sec:autoencoder}
We started with the implementation of \textit{stacked autoencoder}. The library we first started implementing \textit{stacked autoencoder} in was \textbf{Tflearn}. It is a modular deep learning library built on top of Tensorflow, which is designed to provide higher-level API to Tensorflow in order to provide speed in experimentation.\\ \par

The NSL-KDD dataset is already normalized and ready to use. The data set is in comma separated values (csv) format, so for using this dataset we had to use \textit{Pandas}, a python library for manipulating and reading comma separated values files. NSL-KDD dataset is a subset or refinement of KDD Cup 99 dataset has 41 features representing different parameters of network traffic.\\ \par

We had build a model using Tflearn, just like Tensorflow it allows device placement for using multiple CPU/GPU. We started with building the network with input layer to be of 41 neurons as that is the number of input features that NSL-KDD has. Secondly we decided on having three hidden layers with 30, 20 and 10 neurons respectively. The next thing was to identify which activation function to use. We had gone with the most popular \textit{Sigmoid} activation as it is widely popular. So for the three layers we used the \textit{Sigmoid} activation. The final layer or the output layer has five neurons as we have to classify the attacks in five different classes namely, \textit{Normal, DoS, Probing, I2R, U2R}. The activation function for the last layer is \textit{Softmax}, as stated in section \ref{sec:softmax} \textit{Softmax} is used in multi-class classification.\\ \par

After the network is setup, the next task in hand was to define loss function and optimizer function, a \textit{loss function} will calculate the amount to information or data that is lost in correctly identifying. \textit{Optimizer} will try and minimize the loss parameter. For the purpose of this model we have used the \textit{mean square}, which is difference between the estimated values and the original values. After the loss is computed, \textit{Adam Optimizer} will try and minimize the difference which is short for Adaptive Moment Estimation.\\ \par

\section {Data Set Evaluation}\label{sec:datasetEvaluation}

There are 3 datasets available to us in NSL-KDD dataset. A generalized neural network need \textit{train data} for training a neural network, \textit{test data} to evaluate the model with different parameters and a \textit{validation set} to validate the training of a model. Here we have all three dataset with their labels set as well.\\ \par

However, the distribution of the attack types in each are different. In test data set there are total of 22,542 records and in train data set there are 1,25,972 records.  
The distribution of attack types in both test and train set is shown in \ref{table:test_train}\\ \par 
\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Attack Types} & \textbf{Number of records in Train Set} & \textbf{Number of records in Test Set} \\ \hline
Normal                & 67342                                   & 10003                                  \\ \hline
DoS                   & 45927                                   & 7164                                   \\ \hline
Probe                 & 11656                                   & 2421                                   \\ \hline
U2R                   & 995                                     & 2887                                   \\ \hline
I2R                   & 52                                      & 67                                     \\ \hline
\end{tabular}
\caption{Distribution of records in Test and Train Set}
\label{table:test_train}
\end{table}\clearpage

As we can see from the table \ref{test_train} that the attack types are more evenly distributed in test set then in the train set. However, we decided to proceed the conventional way, that is using the data where it is intended to be used. Hence, we started evaluating the model with t

\section {Parameters I}\label{sec:parameters1}
\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
Learning Rate       & 0.01              \\ \hline
Epochs              & 300              \\ \hline
Batch Size          & 100              \\ \hline
Neurons             & {[}30,20,10,5{]} \\ \hline
Activation Function & Sigmoid          \\ \hline
Optimizer           & Adam Optimizer   \\ \hline
\end{tabular}
\caption{Neural Network Parameters for Autoencoder in Tflearn}
\label{tab:nn_tflearn_parameters}
\end{table}

The last layer has \textit{Softmax activation} for the classification. This model did not use the greedy layer-wise training approach, as there was no facility to do so in this library.\\ \par

The number of attacks predicted by the model with the parameters mentioned in table \ref{tab:nn_tflearn_parameters} is shown below.\\ \par

\begin{figure}[h]
\centering
\includegraphics[width=12cm]{confusion_matrix_tflearn__sigmoid_autoencoder.png}
\caption{Predicted Attacks using Sigmoid activation}
\label{graph:auto_tflearn_confusion}
\end{figure}

The classification report with \textit{precision, recall and f1-score} of all the five classes is shown in figure \ref{fig:classification_report} for the parameters mentioned in the table \ref{tab:nn_tflearn_parameters}\\ \par

\begin{figure}[h]
\centering
\includegraphics[width=12cm]{classification_auto__sigmoid_tflearn.png}
\caption{Precision, Recall, F1-score of predicted attacks using Sigmoid activation}
\label{fig:classification_report}
\end{figure}

\section{Parameters II}\label{sec:parameters2}
\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
Learning Rate       & 0.001              \\ \hline
Epochs              & 300              \\ \hline
Batch Size          & 100              \\ \hline
Neurons             & {[}30,20,10,5{]} \\ \hline
Activation Function & ReLU          \\ \hline
Optimizer           & Adam Optimizer   \\ \hline
\end{tabular}
\caption{Neural Network Parameters for Autoencoder in Tflearn}
\label{tab:nn_tflearn_parameters}
\end{table}

Total number of attacks predicted by the model with the parameter described in table \ref{tab:nn_tflearn_parameters} is shown in figure \ref{graph:auto_relu_tflearn_confusion}. The \textit{Precision, Recall, and F1-Score} of the network is shown in graph \ref{graph:auto_relu_tflearn_classification}\\ \par


\clearpage

\begin{figure}[h]
\centering
\includegraphics[width=13cm]{confusion_matrix_tflearn_relu.png}
\caption{Predicted Attacks using ReLU activation using Adam Optimizer}
\label{graph:auto_relu_tflearn_confusion}
\end{figure}

As it is clear from the figure \ref{graph:auto_relu_tflearn_confusion} and tabel \ref{table:total_samples} that there were total of \textit{10003} sample of \textit{Normal} type of attack from which \textit{9200} have been correctly identified.\\ \par

Also the information of how precise the model was to identify the attacks and the recall and f1-score values are also shown in the figure \ref{graph:auto_relu_tflearn_classification}
\begin{figure}[h]
\centering
\includegraphics[width=13cm]{classification_relu_tflearn.png}
\caption{Precision, Recall, F1-score of predicted attacks using ReLU activation using Adam Optimizer}
\label{graph:auto_relu_tflearn_classification}
\end{figure}


\begin{figure}[h]
\centering
\includegraphics[width=13cm]{accuracy_relu_auto_tflearn.png}
\caption{Accuracy of Stacked autoencoder for different activation functions using Adam Optimizer}
\label{graph:auto_accu_tflearn}
\end{figure}
\clearpage

\section{Parameter III}\label{sec:parameter3}

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
Learning Rate       & 0.001              \\ \hline
Epochs              & 300              \\ \hline
Batch Size          & 100              \\ \hline
Neurons             & {[}30,20,10,5{]} \\ \hline
Activation Function & eLU          \\ \hline
Optimizer           & Stochastic Gradient Decent Optimizer   \\ \hline
\end{tabular}
\caption{Parameters of the Stacked Autoencoder}
\label{table:parameters_autoencoder}
\end{table}

The performance of the Stochastic Gradient Decent Optimizer on \textit{Sigmoid, ReLU and Softplus} was below average, but with \textit{eLU} activation it was above average.\\ \par
\begin{figure}[h]
\centering
\includegraphics[width=13cm]{Classification_autoencoder_SGD.png}
\caption{Precision, Recall and F1-Score of Stacked autoencoder with eLU activation and Stochastic Gradient Decent Optimizer}
\label{graph:classificationReportSGDeLU}
\end{figure}
\begin{figure}[h]
\centering
\includegraphics[width=13cm]{confusion_autoencoder_SGD.png}
\caption{Precision, Recall and F1-Score of Stacked autoencoder with eLU activation and Stochastic Gradient Decent Optimizer}
\label{graph:classificationReportSGDeLU}
\end{figure}







\begin{thebibliography}{9}
\bibitem{Kaspersky}
Threat Landscape for Industrial Automation System in Second Half of 2016, Kaspersky Lab ICS CERT.
\bibitem{abs_auto}
Introduction to Autoencoders, David Meyer, January 23, 2015,http://www.1-4-5.net/~dmm/papers/intro_to_autoencoders.pdf
\bibitem{IDS}
INTRUSION DETECTION SYSTEMS;DEFINITION, NEED AND CHALLENGES, SANS Institute 2001,
\bibitem{securityengineering}
Anderson, Ross (2001). Security Engineering: A Guide to Building Dependable Distributed Systems. New York: John Wiley \& Sons. pp. 387–388. ISBN 978-0-471-38922-4.
\bibitem{limitationOfIDS}
Limitations of Network Intrusion Detection, Steve Schupp, December 1, 2000, https://www.giac.org/paper/gsec/235/limitations-network-intrusion-detection/100739
\bibitem{ReLU}
R Hahnloser, R. Sarpeshkar, M A Mahowald, R. J. Douglas, H.S. Seung (2000). Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit. Nature. 405. pp. 947–951.
\bibitem{ELU}
Djork-Arné Clevert, Thomas Unterthiner, Sepp Hochreiter, latest version 22 Feb 2016 (v5).Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs). arXiv:1511.07289 [cs.LG]
\bibitem{unsupervised}
Goodfellow, Ian J.; Pouget-Abadie, Jean; Mirza, Mehdi; Xu, Bing; Warde-Farley, David; Ozair, Sherjil; Courville, Aaron; Bengio, Yoshua (2014). "Generative Adversarial Networks". arXiv:1406.2661
\bibitem{tensorflow_paper}
Large-Scale Machine Learning on Heterogeneous Distributed Systems, Preliminary White Paper, November 9, 2015
\bibitem{googleSearch}
Jack Clark. Google turning its lucrative web search over to AI machines, 2015,www.bloomberg.com/news/articles/2015-10-26/googleturning-its-lucrative-web-search-over-to-ai-machines.
\bibitem{autoencoder}
D.E. Rumelhart, G.E. Hinton, and R.J. Williams. Learning internal representations by error propagation. In Parallel Distributed Processing. Vol 1: Foundations. MIT Press, Cambridge, MA, 1986.
\bibitem{autoencoder2}
Autoencoders, Unsupervised Learning, and Deep Architectures, Pierre Baldi, Department of Computer Science,University of California, Irvine, 2010
\bibitem{ksparseautoencoder}
Alireza Makhzani, Brendan Frey, k-Sparse Autoencoders, 19 Dec 2013, 	arXiv:1312.5663 [cs.LG]
\bibitem{DBN}
Hinton, G. (2009). "Deep belief networks". Scholarpedia. 4 (5): 5947. doi:10.4249/scholarpedia.5947.
\bibitem{RBM}
Smolensky, Paul (1986). "Chapter 6: Information Processing in Dynamical Systems: Foundations of Harmony Theory" (PDF). In Rumelhart, David E.; McLelland, James L. Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1: Foundations. MIT Press. pp. 194–281. ISBN 0-262-68053-X.
\bibitem{RBMuses}
Ruslan Salakhutdinov and Geoffrey Hinton (2010). Replicated softmax: an undirected topic model. Neural Information Processing Systems.
\bibitem{Greedy}
Yoshua Bengio, Pascal Lamblin, Dan Popovici, Hugo Larochelle, Greedy Layer-Wise Training of Deep Networks, Dec 2006, University of Montreal. 
\end{thebibliography}
\end{document}